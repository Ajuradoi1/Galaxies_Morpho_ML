{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b97d9e45-a7c8-4bfd-996d-62f5a632f4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e4df5d8-450b-4787-b06a-eb74fe2ad45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se encontraron 70 archivos TAR.\n"
     ]
    }
   ],
   "source": [
    "# === CONFIGURACIÓN ===\n",
    "tar_folder = \"Images\"          # Carpeta con los archivos TAR\n",
    "pca_output = \"Images_PCA.parquet\"  # Archivo final con PCA\n",
    "n_components = 100                   # Número de componentes PCA\n",
    "batch_size = 1000                     # Lotes de imágenes para procesar en memoria\n",
    "\n",
    "# === RECOGER TODOS LOS TAR ===\n",
    "tar_files = [os.path.join(tar_folder, f) for f in os.listdir(tar_folder) if f.lower().endswith(\".tar\")]\n",
    "print(f\"Se encontraron {len(tar_files)} archivos TAR.\")\n",
    "\n",
    "# === INICIALIZAR ESCALADOR Y PCA INCREMENTAL ===\n",
    "scaler = StandardScaler()\n",
    "ipca = IncrementalPCA(n_components=n_components, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a3c9d9a-f8e9-4654-8d91-ffe8701f87bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculando escalador incremental...\n"
     ]
    }
   ],
   "source": [
    "# === PRIMERA PASADA: AJUSTAR ESCALADOR EN LOTES ===\n",
    "print(\"Calculando escalador incremental...\")\n",
    "\n",
    "X_batch = []\n",
    "for tar_path in tar_files:\n",
    "    with tarfile.open(tar_path, \"r\") as tar:\n",
    "        members = [m for m in tar.getmembers() if m.name.lower().endswith(\".jpg\")]\n",
    "        for member in members:\n",
    "            f = tar.extractfile(member)\n",
    "            if f is not None:\n",
    "                try:\n",
    "                    img = Image.open(BytesIO(f.read())).convert(\"RGB\").resize((64, 64))\n",
    "                    pixels = np.array(img).astype(np.float32).flatten() / 255.0\n",
    "                    X_batch.append(pixels)\n",
    "                    \n",
    "                    if len(X_batch) >= batch_size:\n",
    "                        scaler.partial_fit(np.array(X_batch))\n",
    "                        X_batch = []\n",
    "                except Exception as e:\n",
    "                    print(f\"Error leyendo {member.name}: {e}\")\n",
    "\n",
    "# Procesar último lote\n",
    "if X_batch:\n",
    "    scaler.partial_fit(np.array(X_batch))\n",
    "    X_batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29976326-0bee-44a6-8f3f-a677508d348f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ajustando PCA incremental...\n"
     ]
    }
   ],
   "source": [
    "# === SEGUNDA PASADA: AJUSTAR PCA INCREMENTAL EN LOTES ===\n",
    "print(\"Ajustando PCA incremental...\")\n",
    "\n",
    "X_batch = []\n",
    "for tar_path in tar_files:\n",
    "    with tarfile.open(tar_path, \"r\") as tar:\n",
    "        members = [m for m in tar.getmembers() if m.name.lower().endswith(\".jpg\")]\n",
    "        for member in members:\n",
    "            f = tar.extractfile(member)\n",
    "            if f is not None:\n",
    "                try:\n",
    "                    img = Image.open(BytesIO(f.read())).convert(\"RGB\").resize((64, 64))\n",
    "                    pixels = np.array(img).astype(np.float32).flatten() / 255.0\n",
    "                    X_batch.append(pixels)\n",
    "                    \n",
    "                    if len(X_batch) >= batch_size:\n",
    "                        X_scaled = scaler.transform(np.array(X_batch))\n",
    "                        ipca.partial_fit(X_scaled)\n",
    "                        X_batch = []\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "# Último lote\n",
    "if X_batch:\n",
    "    X_scaled = scaler.transform(np.array(X_batch))\n",
    "    ipca.partial_fit(X_scaled)\n",
    "    X_batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "379bb6ce-eda2-46ff-af21-d257a51a7dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformando imágenes y guardando PCA...\n"
     ]
    }
   ],
   "source": [
    "# === TERCERA PASADA: TRANSFORMAR Y GUARDAR PCA ===\n",
    "print(\"Transformando imágenes y guardando PCA...\")\n",
    "\n",
    "all_filenames = []\n",
    "all_pca = []\n",
    "X_batch = []\n",
    "\n",
    "for tar_path in tar_files:\n",
    "    with tarfile.open(tar_path, \"r\") as tar:\n",
    "        members = [m for m in tar.getmembers() if m.name.lower().endswith(\".jpg\")]\n",
    "        for member in members:\n",
    "            f = tar.extractfile(member)\n",
    "            if f is not None:\n",
    "                try:\n",
    "                    img = Image.open(BytesIO(f.read())).convert(\"RGB\").resize((64, 64))\n",
    "                    pixels = np.array(img).astype(np.float32).flatten() / 255.0\n",
    "                    X_batch.append(pixels)\n",
    "                    # Nombre limpio: solo nombre base sin extensión\n",
    "                    clean_name = os.path.splitext(os.path.basename(member.name))[0]\n",
    "                    all_filenames.append(clean_name)\n",
    "                    \n",
    "                    if len(X_batch) >= batch_size:\n",
    "                        X_scaled = scaler.transform(np.array(X_batch))\n",
    "                        pcs = ipca.transform(X_scaled)\n",
    "                        all_pca.append(pcs)\n",
    "                        X_batch = []\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "# Procesar último lote\n",
    "if X_batch:\n",
    "    X_scaled = scaler.transform(np.array(X_batch))\n",
    "    pcs = ipca.transform(X_scaled)\n",
    "    all_pca.append(pcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4855331c-f354-46a6-97f4-c9d57a16da9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar todos los lotes\n",
    "X_pca = np.vstack(all_pca)\n",
    "pca_columns = [f\"PC{i+1}\" for i in range(n_components)]\n",
    "df_pca = pd.DataFrame(X_pca, columns=pca_columns)\n",
    "df_pca.insert(0, \"filename\", all_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb1a3067-5fb9-4c89-9202-2abf48844f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA completado y guardado en: Images_PCA.parquet\n",
      "Total de imágenes procesadas: 69352\n",
      "Varianza explicada total: 89.16%\n"
     ]
    }
   ],
   "source": [
    "# Guardar PCA final\n",
    "df_pca.to_parquet(pca_output, index=False)\n",
    "\n",
    "print(f\"PCA completado y guardado en: {pca_output}\")\n",
    "print(f\"Total de imágenes procesadas: {len(df_pca)}\")\n",
    "print(f\"Varianza explicada total: {ipca.explained_variance_ratio_.sum():.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fe6335e-df8c-491f-9ef6-c1de2c663a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0            specobjid                objid            dr7objid  \\\n",
      "0           1  1578598304118237184  1237661463301456237  587735742076551517   \n",
      "1           3  1578599678507771904  1237661463301521650  587735742076616950   \n",
      "2           7  1578583460711262208  1237661463301718141  587735742076813455   \n",
      "3          20  1578572190717077504  1237661463302045898  587735742077141189   \n",
      "4          29  1579718437544945664  1237661463302308093  587735742077403410   \n",
      "\n",
      "         ra       dec  p_el_debiased  p_cs_debiased  spiral  elliptical  ...  \\\n",
      "0  233.7615  34.60428          0.000          1.000       1           0  ...   \n",
      "1  233.9483  34.48045          0.069          0.931       1           0  ...   \n",
      "2  234.3422  34.38433          0.015          0.985       1           0  ...   \n",
      "3  235.0977  33.95142          0.040          0.939       1           0  ...   \n",
      "4  235.8138  33.56461          0.000          1.000       1           0  ...   \n",
      "\n",
      "       PC91      PC92      PC93      PC94      PC95      PC96      PC97  \\\n",
      "0 -2.954135  1.089069  0.078552  0.936249  3.116881  2.705702 -1.335519   \n",
      "1 -3.280965 -0.420716 -0.704967  0.204088  1.678300 -0.656912 -2.644598   \n",
      "2  3.748960  0.281389 -1.675344 -2.123224 -0.269120  0.358493  4.744207   \n",
      "3  0.828577  1.086949 -2.727883  0.894270 -2.836356 -2.585843 -0.026283   \n",
      "4 -0.831361 -3.178203 -0.329741  2.395121  1.160826 -1.092799 -1.353875   \n",
      "\n",
      "       PC98      PC99     PC100  \n",
      "0  2.555136  0.077765  1.330935  \n",
      "1 -0.866718  0.656510 -2.160167  \n",
      "2 -1.596801 -1.923029  1.200982  \n",
      "3 -1.036435 -0.620209 -3.596934  \n",
      "4  3.142564 -4.112560  2.761319  \n",
      "\n",
      "[5 rows x 122 columns]\n",
      "(69964, 122)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Leer CSV y PCA\n",
    "df_catalog = pd.read_csv(\"ZooSpecPhotoDR19_filtered.csv\")\n",
    "df_pca = pd.read_parquet(\"Images_PCA.parquet\")\n",
    "\n",
    "# Convertir ambos a string\n",
    "df_catalog[\"objid\"] = df_catalog[\"objid\"].astype(str)\n",
    "df_pca[\"filename\"] = df_pca[\"filename\"].astype(str)\n",
    "\n",
    "# Merge\n",
    "df_final = pd.merge(df_catalog, df_pca, left_on=\"objid\", right_on=\"filename\", how=\"inner\")\n",
    "\n",
    "# Opcional: eliminar columna filename duplicada\n",
    "df_final = df_final.drop(columns=[\"filename\"])\n",
    "\n",
    "print(df_final.head())\n",
    "print(df_final.shape)\n",
    "\n",
    "# Guardar dataset combinado\n",
    "df_final.to_parquet(\"Dataset_combinado.parquet\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
