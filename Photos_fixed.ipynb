{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24097c54-d680-4107-9cd1-132e29e96520",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import time\n",
    "import tarfile\n",
    "import io\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from urllib.parse import urlencode\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89716343-937e-44f5-8c5f-8d3c257bb149",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read CSV\n",
    "\n",
    "df = pd.read_csv(\"ZooSpecPhotoDR19_filtered.csv\") #Shape (69352, 22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d013ff4-e7a1-48d6-b9d0-27c5ebcb94c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's work with Hips2Fits\n",
    "\n",
    "HIPS_URL = \"https://alasky.cds.unistra.fr/hips-image-services/hips2fits\"\n",
    "\n",
    "#We need to access the Hips2Fits URL and look for what we want. We will need a function for that.\n",
    "#Compulsory input parameters: hips, width, height, projection, fov, ra, dec.\n",
    "\n",
    "def Hips2Fits_access(ra, dec, petroR90_r, width=64, height=64, hips=\"CDS/P/SDSS9/color\"):\n",
    "    \n",
    "    #As we could see, one of the compulsory input parameters was the FOV. We need to optimize it with the following equation:\n",
    "    #FOV = 2*petroR90_r_deg*(1+margin_base)\n",
    "    \n",
    "    #We are using margin_base = 0.25 (25%) as default.\n",
    "    #We also have to be careful. Hips2Fits works with degreen when it comes to FOV. As we are using petroR90_r in\n",
    "    #the equation and the csv works with arcsecs for it, we need to change units!\n",
    "\n",
    "    petroR90_r_deg = float(petroR90_r)/3600.0  #from arcsecs to deg\n",
    "    FOV = 2*petroR90_r_deg*(1+0.25)            \n",
    "\n",
    "    #Request for an especific galaxy using everything we have mentioned\n",
    "    params = {\n",
    "    \"hips\": hips,\n",
    "    \"ra\": ra,\n",
    "    \"dec\": dec,\n",
    "    \"width\": width, #pixels\n",
    "    \"height\": height, #pixels\n",
    "    \"fov\": FOV,\n",
    "    \"projection\": \"TAN\", #Minimal distortions for tangential projection\n",
    "    \"format\": \"jpg\" #Format of the images we are getting. It can be jpg, png or fits.\n",
    "    }\n",
    "\n",
    "    return (f\"{HIPS_URL}?{urlencode(params)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2897d2bc-f4ce-459d-be25-ccb887843f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Once we know where to look for, it is time for the storage part\n",
    "\n",
    "Output_Dir = Path(\"Images\") #Output directory, name of the folder we are shaving everything\n",
    "Output_Dir.mkdir(exist_ok=True) #Making the directory, and in case it exist, an error won't appear\n",
    "\n",
    "#The 'problem' here is that we have MORE than 60k images. We will use TAR files to save them. The reason is simple:\n",
    "#we can save thousands of images in one TAR file, and it will count as just one object instead of thousands.\n",
    "#Even though it might be tougher to grab just one image, when talking about storing and transferring datasets\n",
    "#it is way more efficient.\n",
    "\n",
    "Images_per_TAR = 1000\n",
    "\n",
    "Images_metadata= [] #Metadata about the downloaded images will be saved here\n",
    "\n",
    "#We create an empty list at the beginning. Each added element is a triplet (3-tuple) of three elements:\n",
    "# - Filename: the desired name of the image inside the TAR.\n",
    "# - Bytes: real binary data of the image downloaded with \"requests\" (better explained later)\n",
    "# - Extra: we want to associate the data we have in the CSV to our images.\n",
    "\n",
    "Images_buffer = []  # [(filename, bytes, extra), ...]\n",
    "TAR_index = 0 #To count from the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1889dee1-eb2c-4e5c-8135-be8af82cf586",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We know where to find the images, how to store it and the request part. It is time to save the images in our TAR files\n",
    "\n",
    "def save_images_to_TAR(TAR_path, contents):\n",
    "    with tarfile.open(TAR_path, \"w\") as tar: #The \"w\" is from \"write\". It creates a new TAR (eliminates if it already exists)\n",
    "        #The \"with\" command is called context manager. Its main purpouse is to open a resource (in our case a TAR file),\n",
    "        #run the block and close the file properly even if there is an error. \n",
    "        for filename, bits, extra in contents:\n",
    "            #Be careful because TAR files don't accept bytes directly but something that behaves as a \"file-like object\".\n",
    "            #That is why we use io.BytesIO. It creates temporal files in RAM memory from these bytes.\n",
    "            File_obj = io.BytesIO(bits)\n",
    "\n",
    "            #We move the \"cursor\" to the beginning. When opening or creating a file, Python has an inner \"cursor\"\n",
    "            #that indicates where in the file you are. Every time you read or write, the \"cursor\" moves forward.\n",
    "            #That is why we need to use the .seek(0) command, so we guarantee that the \"cursor\" is in the beginning\n",
    "            #before doing anything.\n",
    "            File_obj.seek(0)\n",
    "            \n",
    "            TAR_info = tarfile.TarInfo(name=f\"{filename}.jpg\")\n",
    "            TAR_info.size = len(bits) #Add the size info to TAR_info\n",
    "            tar.addfile(TAR_info, File_obj)\n",
    "    print(\"TAR saved in:\", TAR_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3879f8be-c493-4f4e-b21d-b35186edf6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#When using parallelization we need to control the amount of attempts per image, retry in case of error and quit\n",
    "#after several failed attempts so we don't get stucked.\n",
    "\n",
    "#When accessing Hits2Fits, as we have a lot of requests, we won't be doing it one by one. For that, we need to keep a 'session' open\n",
    "#all the time. It will save up time. How do we do that? With Python’s standard library \"requests\" for HTTP requests.\n",
    "\n",
    "session = requests.Session() #Creates a persistent session object that maintains certain things between requests:\n",
    "                            #  - Persistent TCP connection to avoid repeated handshake for each image.\n",
    "                            #  - Cookies to be reused automatically if the server uses them for authentication.\n",
    "\n",
    "def download_with_retries(ra, dec, petroR90_r, max_retries=5, delay=2):\n",
    "    URL = Hips2Fits_access(ra, dec, petroR90_r) #URL of the desired image to download.\n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            response = session.get(URL, timeout=30) #We get into our session (defined with the request.Session())\n",
    "            if response.status_code == 200: #This code means the download has succeded \n",
    "                return response.content #Return the bytes\n",
    "            else:\n",
    "                print(f\"[{attempt}/{max_retries}] Error {response.status_code} in {URL}\")\n",
    "        except requests.RequestException as e: #These are type of error that aren't given in the form of ERROR #NUMBER. These\n",
    "                                               #most of the times are associated with connection errors.\n",
    "            print(f\"[{attempt}/{max_retries}] Connection error: {e}\")\n",
    "        if attempt < max_retries:\n",
    "            time.sleep(delay)  #Wait before the following attempt\n",
    "    print(f\"Permanent error after {max_retries} retries: {URL}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aa7fcf-d5b9-4ebc-8ac9-c3f1cca216ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parallel execution\n",
    "\n",
    "#As we don't want to be working TAR by TAR, we can parallelize. Parallelization is the process of running multiple tasks\n",
    "#simultaneously instead of sequentially.\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=20) as executor: #Up to 20 images downloaded at the same time.\n",
    "    \n",
    "    #Let's write a list of parallel tasks (in our case, image downloading). We have to use a dictionary to avoid errors:\n",
    "    futures = {executor.submit(download_with_retries, t.ra, t.dec, t.petroR90_r): t\n",
    "    for t in df.itertuples()}\n",
    "    \n",
    "    #itertuples() reiterate in every row of the DataFrame. Each row is converted in a namedtuple,\n",
    "    #which means that we have access to the columns by name or position.\n",
    "\n",
    "    completed = 0 #We start the counter\n",
    "    \n",
    "    for f in as_completed(futures): #as_completed(futures) is an iterator that returns the 'futures' (our lists) as they finish.\n",
    "        List = futures[f]\n",
    "        bits = f.result() \n",
    "        if bits is not None:\n",
    "            filename = str(List.objid)\n",
    "            extra = {\n",
    "                \"id\": List.objid,\n",
    "                \"ra\": List.ra,\n",
    "                \"dec\": List.dec,\n",
    "                \"spiral\": getattr(List, \"spiral\", None),\n",
    "                \"elliptical\": getattr(List, \"elliptical\", None),\n",
    "            }\n",
    "            Images_buffer.append((filename,bits,extra))\n",
    "            Images_metadata.append({\n",
    "                \"id\": List.objid,\n",
    "                \"ra\": List.ra,\n",
    "                \"dec\": List.dec,\n",
    "                \"spiral\": getattr(List, \"spiral\", None),\n",
    "                \"elliptical\": getattr(List, \"elliptical\", None),\n",
    "                \"shard\": f\"shard-{TAR_index:06d}.tar\", #Decimal format with 6 digits, adding zeros on the left if it’s shorter.\n",
    "                \"member\": f\"{filename}.jpg\"\n",
    "            })\n",
    "        else:\n",
    "            continue #We don't want everything to stop\n",
    "        completed += 1\n",
    "\n",
    "        if completed % 100 == 0: #completed % 100 calculates the remainder of that division. In other words: if it's 100% completed.\n",
    "            print(\"We got this!\")\n",
    "\n",
    "        if len(Images_buffer) >= Images_per_TAR: #If the TAR is full\n",
    "            save_images_to_TAR(Output_Dir / f\"shard-{TAR_index:06d}.tar\", Images_buffer) # creates the TAR file on your disk\n",
    "            Images_buffer = [] #We empty it so we can start filling again\n",
    "            TAR_index += 1 # So the following TAR gets the next number\n",
    "\n",
    "#We write last shard if not empty\n",
    "if Images_buffer: #Checks if the list is not empty.\n",
    "    save_images_to_TAR(Output_Dir/ f\"shard-{TAR_index:06d}.tar\", Images_buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2034b910-d718-4487-99e5-fc9bbbdd84f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We have finished with the downloads. Now let's save the metadata we got as a compressed CSV\n",
    "\n",
    "metadata_df = pd.DataFrame(Images_metadata)\n",
    "\n",
    "#Making the directory, and in case it exist, an error won't appear\n",
    "Output_Dir.mkdir(parents=True, exist_ok=True) #The command parents=True creates all the missing parent directories in the path.\n",
    "\n",
    "metadata_df.to_csv(Output_Dir / \"metadata.csv.gz\", index=False, compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67be4498-208f-44a8-8856-7f2eef82e7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's see if we can plot any image\n",
    "\n",
    "#We are openning the first TAR (for example)\n",
    "tar_path = \"Images/TAR-000000.tar\"\n",
    "with tarfile.open(tar_path, \"r\") as tar:\n",
    "    # list contents\n",
    "    members = tar.getmembers()\n",
    "    # extract the first cutout\n",
    "    img_member = members[0]\n",
    "    img_bytes = tar.extractfile(img_member).read()\n",
    "\n",
    "# show the image\n",
    "img = Image.open(io.BytesIO(img_bytes)) \n",
    "plt.imshow(img)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
