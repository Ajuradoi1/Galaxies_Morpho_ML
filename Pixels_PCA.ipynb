{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b97d9e45-a7c8-4bfd-996d-62f5a632f4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e4df5d8-450b-4787-b06a-eb74fe2ad45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se encontraron 70 archivos TAR.\n"
     ]
    }
   ],
   "source": [
    "tar_folder = \"Images\"          #Folder with the TAR files\n",
    "pca_output = \"Images_PCA.parquet\"  #Final file with PCA\n",
    "n_components = 100                   #PCA number of components. More number, less compression, better quality\n",
    "batch_size = 1000                     #Set of images to process in memory\n",
    "\n",
    "scaler = StandardScaler() #PCA depends on variance: if there is one characteristic with bigger values,\n",
    "                          #it would dominate. In order to prevent that, we scalate so everything would contribute the same\n",
    "\n",
    "ipca = IncrementalPCA(n_components=n_components, batch_size=batch_size) #Inicializes an incremental PCA.\n",
    "# It allows processing large volumes of data in batches, without loading everything into memory at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a3c9d9a-f8e9-4654-8d91-ffe8701f87bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculando escalador incremental...\n"
     ]
    }
   ],
   "source": [
    "#FIRST PASS of preprocessing before applying PCA: Calculate the mean and global standard devitation of all pixels\n",
    "\n",
    "X_batch = [] #Creates an empty list to temporarily store pixel vectors for each batch.\n",
    "for tar_path in tar_files:\n",
    "    with tarfile.open(tar_path, \"r\") as tar:\n",
    "        members = [m for m in tar.getmembers() if m.name.lower().endswith(\".jpg\")] #Filters only the .jpg images inside the archive.\n",
    "        for member in members:\n",
    "            f = tar.extractfile(member) #Extracts the current image file as a binary object in memory.\n",
    "            if f is not None: #Checking if the image is successfully extracted.\n",
    "                try:\n",
    "                    img = Image.open(BytesIO(f.read())).convert(\"RGB\").resize((64, 64)) #Reads the image bytes into memory, opens it,\n",
    "                                                                                        #ensures 3 color channels and resizes to a fixed shape\n",
    "                    pixels = np.array(img).astype(np.float32).flatten() / 255.0 #Converts the image into a NumPy array, flattens it from\n",
    "                                                                                #64×64×3=12288-length vector and normalizes pixel values to\n",
    "                                                                                # the range [0, 1].\n",
    "                                                                                #It makes values float32 (needed by scaler/IPCA).\n",
    "                    X_batch.append(pixels) #Adds the pixel vector to the current batch.\n",
    "                    \n",
    "                    if len(X_batch) >= batch_size:\n",
    "                        scaler.partial_fit(np.array(X_batch))  #When the batch is full, scaler.partial_fit() updates the mean and standard\n",
    "                                                                #deviation for scaling using just that batch.\n",
    "\n",
    "                        X_batch = [] #It clears X_batch to start collecting a new batch.\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {member.name}: {e}\")\n",
    "\n",
    "#Processing the last set\n",
    "if X_batch:\n",
    "    scaler.partial_fit(np.array(X_batch))\n",
    "    X_batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29976326-0bee-44a6-8f3f-a677508d348f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ajustando PCA incremental...\n"
     ]
    }
   ],
   "source": [
    "#SECOND PASS adjusting incremental PCA in sets: scalates all the data and trains IncrementalPCA\n",
    "\n",
    "X_batch = []\n",
    "for tar_path in tar_files:\n",
    "    with tarfile.open(tar_path, \"r\") as tar:\n",
    "        members = [m for m in tar.getmembers() if m.name.lower().endswith(\".jpg\")]\n",
    "        for member in members:\n",
    "            f = tar.extractfile(member)\n",
    "            if f is not None:\n",
    "                try:\n",
    "                    img = Image.open(BytesIO(f.read())).convert(\"RGB\").resize((64, 64))\n",
    "                    pixels = np.array(img).astype(np.float32).flatten() / 255.0\n",
    "                    X_batch.append(pixels)\n",
    "                    \n",
    "                    if len(X_batch) >= batch_size:\n",
    "                        X_scaled = scaler.transform(np.array(X_batch)) #Convert the list to a NumPy array of shape\n",
    "                                                                       #(n_samples_in_batch, n_features) and standardize it using the\n",
    "                                                                       #scaler fitted during the first pass. This centers each feature\n",
    "                                                                       #to mean 0 and scales to unit variance\n",
    "                                                                       #(using global mean/std learned earlier).\n",
    "                        ipca.partial_fit(X_scaled) #Feed the standardized batch to the IncrementalPCA. partial_fit() updates the PCA internal\n",
    "                                                   #state (estimates of components, means...) incrementally. Each call refines the global PCA\n",
    "                                                   #using the new batch.\n",
    "                        X_batch = []\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "#Last set\n",
    "if X_batch:\n",
    "    X_scaled = scaler.transform(np.array(X_batch))\n",
    "    ipca.partial_fit(X_scaled)\n",
    "    X_batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af25e85d-2949-4e82-a5e8-a70d0cac8f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These two codes can't merge (even if they seem to be very similar)\n",
    "\n",
    "#When using \"StandardScaler\", it needs to know the global mean and global standard deviation of the entire dataset\n",
    "#in order to scale the data correctly. The problem is that during the first pass, those statistics are not yet known.\n",
    "#They are still being computed gradually. If \"scaler.transform()\" is applied at the same time as \"scaler.partial_fit()\",\n",
    "#you would be scaling using partial and changing values, not the final mean and standard deviation.\n",
    "#This would cause each batch to be scaled differently and if the data aren’t scaled consistently, the PCA would learn\n",
    "#distorted or incorrect components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "379bb6ce-eda2-46ff-af21-d257a51a7dfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformando imágenes y guardando PCA...\n"
     ]
    }
   ],
   "source": [
    "#THIRD PASS transforming and saving PCA\n",
    "\n",
    "all_filenames = []\n",
    "all_pca = []\n",
    "X_batch = []\n",
    "\n",
    "for tar_path in tar_files:\n",
    "    with tarfile.open(tar_path, \"r\") as tar:\n",
    "        members = [m for m in tar.getmembers() if m.name.lower().endswith(\".jpg\")]\n",
    "        for member in members:\n",
    "            f = tar.extractfile(member)\n",
    "            if f is not None:\n",
    "                try:\n",
    "                    img = Image.open(BytesIO(f.read())).convert(\"RGB\").resize((64, 64))\n",
    "                    pixels = np.array(img).astype(np.float32).flatten() / 255.0\n",
    "                    X_batch.append(pixels)\n",
    "                    #Clean name: just base name without extention\n",
    "                    clean_name = os.path.splitext(os.path.basename(member.name))[0] #Create a “clean” identifier\n",
    "                                                                                    #(filename without directory or extension) for the image.\n",
    "                    all_filenames.append(clean_name) #Append that clean name to the global filename list immediately\n",
    "                    \n",
    "                    if len(X_batch) >= batch_size:\n",
    "                        X_scaled = scaler.transform(np.array(X_batch)) #Use the already-fitted StandardScaler (from FIRST PASS)\n",
    "                                                                       #to standardize the batch.\n",
    "                        pcs = ipca.transform(X_scaled) #Use the already-fitted IncrementalPCA (from SECOND PASS) to project\n",
    "                                                       #the standardized batch into PCA\n",
    "                        all_pca.append(pcs)\n",
    "                        X_batch = []\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "#Last set\n",
    "if X_batch:\n",
    "    X_scaled = scaler.transform(np.array(X_batch))\n",
    "    pcs = ipca.transform(X_scaled)\n",
    "    all_pca.append(pcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4855331c-f354-46a6-97f4-c9d57a16da9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging all the sets\n",
    "X_pca = np.vstack(all_pca)\n",
    "pca_columns = [f\"PC{i+1}\" for i in range(n_components)]\n",
    "df_pca = pd.DataFrame(X_pca, columns=pca_columns)\n",
    "df_pca.insert(0, \"filename\", all_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb1a3067-5fb9-4c89-9202-2abf48844f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA completado y guardado en: Images_PCA.parquet\n",
      "Total de imágenes procesadas: 69352\n",
      "Varianza explicada total: 89.16%\n"
     ]
    }
   ],
   "source": [
    "# Save final PCA\n",
    "df_pca.to_parquet(pca_output, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fe6335e-df8c-491f-9ef6-c1de2c663a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0            specobjid                objid            dr7objid  \\\n",
      "0           1  1578598304118237184  1237661463301456237  587735742076551517   \n",
      "1           3  1578599678507771904  1237661463301521650  587735742076616950   \n",
      "2           7  1578583460711262208  1237661463301718141  587735742076813455   \n",
      "3          20  1578572190717077504  1237661463302045898  587735742077141189   \n",
      "4          29  1579718437544945664  1237661463302308093  587735742077403410   \n",
      "\n",
      "         ra       dec  p_el_debiased  p_cs_debiased  spiral  elliptical  ...  \\\n",
      "0  233.7615  34.60428          0.000          1.000       1           0  ...   \n",
      "1  233.9483  34.48045          0.069          0.931       1           0  ...   \n",
      "2  234.3422  34.38433          0.015          0.985       1           0  ...   \n",
      "3  235.0977  33.95142          0.040          0.939       1           0  ...   \n",
      "4  235.8138  33.56461          0.000          1.000       1           0  ...   \n",
      "\n",
      "       PC91      PC92      PC93      PC94      PC95      PC96      PC97  \\\n",
      "0 -2.954135  1.089069  0.078552  0.936249  3.116881  2.705702 -1.335519   \n",
      "1 -3.280965 -0.420716 -0.704967  0.204088  1.678300 -0.656912 -2.644598   \n",
      "2  3.748960  0.281389 -1.675344 -2.123224 -0.269120  0.358493  4.744207   \n",
      "3  0.828577  1.086949 -2.727883  0.894270 -2.836356 -2.585843 -0.026283   \n",
      "4 -0.831361 -3.178203 -0.329741  2.395121  1.160826 -1.092799 -1.353875   \n",
      "\n",
      "       PC98      PC99     PC100  \n",
      "0  2.555136  0.077765  1.330935  \n",
      "1 -0.866718  0.656510 -2.160167  \n",
      "2 -1.596801 -1.923029  1.200982  \n",
      "3 -1.036435 -0.620209 -3.596934  \n",
      "4  3.142564 -4.112560  2.761319  \n",
      "\n",
      "[5 rows x 122 columns]\n",
      "(69964, 122)\n"
     ]
    }
   ],
   "source": [
    "#Read CSV and PCA\n",
    "df_catalog = pd.read_csv(\"ZooSpecPhotoDR19_filtered.csv\")\n",
    "df_pca = pd.read_parquet(\"Images_PCA.parquet\")\n",
    "\n",
    "#Convert both to string so we can compare the 'objid' columns with the names of the images (saved as filename in the PCA)\n",
    "df_catalog[\"objid\"] = df_catalog[\"objid\"].astype(str)\n",
    "df_pca[\"filename\"] = df_pca[\"filename\"].astype(str)\n",
    "\n",
    "# Merge\n",
    "df_final = pd.merge(df_catalog, df_pca, left_on=\"objid\", right_on=\"filename\", how=\"inner\")\n",
    "\n",
    "#Eliminate the 'filename' column (duplicated)\n",
    "df_final = df_final.drop(columns=[\"filename\"])\n",
    "\n",
    "#Save combinated dataset\n",
    "df_final.to_parquet(\"Dataset_combinado.parquet\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
