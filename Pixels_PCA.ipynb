{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b97d9e45-a7c8-4bfd-996d-62f5a632f4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0e4df5d8-450b-4787-b06a-eb74fe2ad45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_folder = \"Images\"          #Folder with the TAR files\n",
    "tar_files = [os.path.join(tar_folder, f) for f in os.listdir(tar_folder) if f.endswith('.tar')] #List of TAR files\n",
    "pca_output = \"Images_PCA.parquet\"  #Final file with PCA\n",
    "n_components = 100                   #PCA number of components. More number, less compression, better quality\n",
    "batch_size = 1000                     #Set of images to process in memory\n",
    "\n",
    "scaler = StandardScaler() #PCA depends on variance: if there is one characteristic with bigger values,\n",
    "                          #it would dominate. In order to prevent that, we scalate so everything would contribute the same\n",
    "\n",
    "ipca = IncrementalPCA(n_components=n_components, batch_size=batch_size) #Inicializes an incremental PCA.\n",
    "# It allows processing large volumes of data in batches, without loading everything into memory at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a3c9d9a-f8e9-4654-8d91-ffe8701f87bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FIRST PASS of preprocessing before applying PCA: Calculate the mean and global standard devitation of all pixels\n",
    "\n",
    "X_batch = [] #Creates an empty list to temporarily store pixel vectors for each batch.\n",
    "for tar_path in tar_files:\n",
    "    with tarfile.open(tar_path, \"r\") as tar:\n",
    "        members = [m for m in tar.getmembers() if m.name.lower().endswith(\".jpg\")] #Filters only the .jpg images inside the archive.\n",
    "        for member in members:\n",
    "            f = tar.extractfile(member) #Extracts the current image file as a binary object in memory.\n",
    "            if f is not None: #Checking if the image is successfully extracted.\n",
    "                try:\n",
    "                    img = Image.open(BytesIO(f.read())).convert(\"RGB\").resize((64, 64)) #Reads the image bytes into memory, opens it,\n",
    "                                                                                        #ensures 3 color channels and resizes to a fixed shape\n",
    "                    pixels = np.array(img).astype(np.float32).flatten() / 255.0 #Converts the image into a NumPy array, flattens it from\n",
    "                                                                                #64×64×3=12288-length vector and normalizes pixel values to\n",
    "                                                                                # the range [0, 1].\n",
    "                                                                                #It makes values float32 (needed by scaler/IPCA).\n",
    "                    X_batch.append(pixels) #Adds the pixel vector to the current batch.\n",
    "                    \n",
    "                    if len(X_batch) >= batch_size:\n",
    "                        scaler.partial_fit(np.array(X_batch))  #When the batch is full, scaler.partial_fit() updates the mean and standard\n",
    "                                                                #deviation for scaling using just that batch.\n",
    "\n",
    "                        X_batch = [] #It clears X_batch to start collecting a new batch.\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading {member.name}: {e}\")\n",
    "\n",
    "#Processing the last set\n",
    "if X_batch:\n",
    "    scaler.partial_fit(np.array(X_batch))\n",
    "    X_batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "29976326-0bee-44a6-8f3f-a677508d348f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "#SECOND PASS adjusting incremental PCA in sets: scalates all the data and trains IncrementalPCA\n",
    "\n",
    "X_batch = []\n",
    "for tar_path in tar_files:\n",
    "    with tarfile.open(tar_path, \"r\") as tar:\n",
    "        members = [m for m in tar.getmembers() if m.name.lower().endswith(\".jpg\")]\n",
    "        for member in members:\n",
    "            f = tar.extractfile(member)\n",
    "            if f is not None:\n",
    "                try:\n",
    "                    img = Image.open(BytesIO(f.read())).convert(\"RGB\").resize((64, 64))\n",
    "                    pixels = np.array(img).astype(np.float32).flatten() / 255.0\n",
    "                    X_batch.append(pixels)\n",
    "                    \n",
    "                    if len(X_batch) >= batch_size:\n",
    "                        X_scaled = scaler.transform(np.array(X_batch)) #Convert the list to a NumPy array of shape\n",
    "                                                                       #(n_samples_in_batch, n_features) and standardize it using the\n",
    "                                                                       #scaler fitted during the first pass. This centers each feature\n",
    "                                                                       #to mean 0 and scales to unit variance\n",
    "                                                                       #(using global mean/std learned earlier).\n",
    "                        ipca.partial_fit(X_scaled) #Feed the standardized batch to the IncrementalPCA. partial_fit() updates the PCA internal\n",
    "                                                   #state (estimates of components, means...) incrementally. Each call refines the global PCA\n",
    "                                                   #using the new batch.\n",
    "                        X_batch = []\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "print(len(X_batch))\n",
    "\n",
    "#Last set\n",
    "if X_batch:\n",
    "    X_scaled = scaler.transform(np.array(X_batch))\n",
    "    ipca.partial_fit(X_scaled)\n",
    "    X_batch = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "af25e85d-2949-4e82-a5e8-a70d0cac8f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#These two codes can't merge (even if they seem to be very similar)\n",
    "\n",
    "#When using \"StandardScaler\", it needs to know the global mean and global standard deviation of the entire dataset\n",
    "#in order to scale the data correctly. The problem is that during the first pass, those statistics are not yet known.\n",
    "#They are still being computed gradually. If \"scaler.transform()\" is applied at the same time as \"scaler.partial_fit()\",\n",
    "#you would be scaling using partial and changing values, not the final mean and standard deviation.\n",
    "#This would cause each batch to be scaled differently and if the data aren’t scaled consistently, the PCA would learn\n",
    "#distorted or incorrect components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "379bb6ce-eda2-46ff-af21-d257a51a7dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIRD PASS transforming and saving PCA\n",
    "\n",
    "all_filenames = []\n",
    "all_pca = []\n",
    "X_batch = []\n",
    "\n",
    "for tar_path in tar_files:\n",
    "    with tarfile.open(tar_path, \"r\") as tar:\n",
    "        members = [m for m in tar.getmembers() if m.name.lower().endswith(\".jpg\")]\n",
    "        for member in members:\n",
    "            f = tar.extractfile(member)\n",
    "            if f is not None:\n",
    "                try:\n",
    "                    img = Image.open(BytesIO(f.read())).convert(\"RGB\").resize((64, 64))\n",
    "                    pixels = np.array(img).astype(np.float32).flatten() / 255.0\n",
    "                    X_batch.append(pixels)\n",
    "                    #Clean name: just base name without extention\n",
    "                    clean_name = os.path.splitext(os.path.basename(member.name))[0] #Create a “clean” identifier\n",
    "                                                                                    #(filename without directory or extension) for the image.\n",
    "                    all_filenames.append(clean_name) #Append that clean name to the global filename list immediately\n",
    "                    \n",
    "                    if len(X_batch) >= batch_size:\n",
    "                        X_scaled = scaler.transform(np.array(X_batch)) #Use the already-fitted StandardScaler (from FIRST PASS)\n",
    "                                                                       #to standardize the batch.\n",
    "                        pcs = ipca.transform(X_scaled) #Use the already-fitted IncrementalPCA (from SECOND PASS) to project\n",
    "                                                       #the standardized batch into PCA\n",
    "                        all_pca.append(pcs)\n",
    "                        X_batch = []\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "#Last set\n",
    "if X_batch:\n",
    "    X_scaled = scaler.transform(np.array(X_batch))\n",
    "    pcs = ipca.transform(X_scaled)\n",
    "    all_pca.append(pcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4855331c-f354-46a6-97f4-c9d57a16da9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Merging all the sets\n",
    "X_pca = np.vstack(all_pca)\n",
    "pca_columns = [f\"PC{i+1}\" for i in range(n_components)]\n",
    "df_pca = pd.DataFrame(X_pca, columns=pca_columns)\n",
    "df_pca.insert(0, \"filename\", all_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eb1a3067-5fb9-4c89-9202-2abf48844f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final PCA\n",
    "df_pca.to_parquet(pca_output, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2fe6335e-df8c-491f-9ef6-c1de2c663a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read CSV and PCA\n",
    "df_catalog = pd.read_csv(\"ZooSpecPhotoDR19_filtered.csv.gz\", compression='gzip')\n",
    "df_pca = pd.read_parquet(\"Images_PCA.parquet\")\n",
    "\n",
    "#Convert both to string so we can compare the 'objid' columns with the names of the images (saved as filename in the PCA)\n",
    "df_catalog[\"objid\"] = df_catalog[\"objid\"].astype(str)\n",
    "df_pca[\"filename\"] = df_pca[\"filename\"].astype(str)\n",
    "\n",
    "# Merge\n",
    "df_final = pd.merge(df_catalog, df_pca, left_on=\"objid\", right_on=\"filename\", how=\"inner\")\n",
    "\n",
    "#Eliminate the 'filename' column (duplicated)\n",
    "df_final = df_final.drop(columns=[\"filename\"])\n",
    "\n",
    "#Save combinated dataset\n",
    "df_final.to_parquet(\"Dataset_combinado.parquet\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
